[1] "xgboost_small_pt_4.R"
[1] "Copying model file..."
[1] "Pre-processing..."
[1] "...Done!"
[1] "Running the model..."
Aggregating results
Selecting tuning parameters
Fitting nrounds = 110, max_depth = 4, eta = 0.1, gamma = 0, colsample_bytree = 0.5, min_child_weight = 4, subsample = 0.8 on full training set
[1] "...Done!"
eXtreme Gradient Boosting 

1508 samples
 158 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 1356, 1357, 1358, 1357, 1357, 1358, ... 
Resampling results across tuning parameters:

  max_depth  min_child_weight  MAE     
  4          4                 1297.841
  4          5                 1312.299
  4          6                 1325.946
  5          4                 1324.532
  5          5                 1305.943
  5          6                 1310.451
  6          4                 1352.017
  6          5                 1324.565
  6          6                 1322.147

Tuning parameter 'nrounds' was held constant at a value of 110
Tuning
 'colsample_bytree' was held constant at a value of 0.5
Tuning
 parameter 'subsample' was held constant at a value of 0.8
MAE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 110, max_depth = 4, eta
 = 0.1, gamma = 0, colsample_bytree = 0.5, min_child_weight = 4 and subsample
 = 0.8. 
[1] "Estimated RMSE:"
        RMSE     Rsquared 
2249.9297399    0.4502212 
[1] "Estimated MAE:"
[1] 1297.478
[1] "Best Parameters:"
  nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
1     110         4 0.1     0              0.5                4       0.8
[1] "Run time:"
   user  system elapsed 
  6.699   1.352  55.212 
