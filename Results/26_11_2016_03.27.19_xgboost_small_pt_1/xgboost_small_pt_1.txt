[1] "xgboost_small_pt_1.R"
[1] "Copying model file..."
Read 95.6% of 188318 rowsRead 188318 rows and 78 (of 132) columns from 0.065 GB file in 00:00:03
[1] "Pre-processing..."
[1] "...Done!"
[1] "Running the model..."
Aggregating results
Selecting tuning parameters
Fitting nrounds = 100, max_depth = 5, eta = 0.1, gamma = 0, colsample_bytree = 0.5, min_child_weight = 1, subsample = 0.8 on full training set
[1] "...Done!"
eXtreme Gradient Boosting 

1508 samples
 158 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 1356, 1357, 1358, 1357, 1357, 1358, ... 
Resampling results across tuning parameters:

  nrounds  MAE     
   100     1328.774
   200     1342.020
   300     1352.490
   400     1356.369
   500     1356.820
   600     1358.504
   700     1359.709
   800     1360.166
   900     1360.788
  1000     1360.985

Tuning parameter 'max_depth' was held constant at a value of 5
Tuning
 parameter 'min_child_weight' was held constant at a value of 1

Tuning parameter 'subsample' was held constant at a value of 0.8
MAE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 100, max_depth = 5, eta
 = 0.1, gamma = 0, colsample_bytree = 0.5, min_child_weight = 1 and subsample
 = 0.8. 
[1] "Estimated RMSE:"
        RMSE     Rsquared 
2242.3817734    0.4611718 
[1] "Estimated MAE:"
[1] 1294.217
[1] "Best Parameters:"
  nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
1     100         5 0.1     0              0.5                1       0.8
[1] "Run time:"
   user  system elapsed 
  6.365   1.310  54.889 
