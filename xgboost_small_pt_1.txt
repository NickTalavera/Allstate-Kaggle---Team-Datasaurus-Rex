[1] "xgboost_small_pt_1.R"
[1] "Copying model file..."
Read 95.6% of 188318 rowsRead 188318 rows and 78 (of 132) columns from 0.065 GB file in 00:00:03
[1] "Pre-processing..."
[1] "...Done!"
[1] "Running the model..."
Aggregating results
Selecting tuning parameters
Fitting nrounds = 200, max_depth = 5, eta = 0.1, gamma = 0, colsample_bytree = 0.5, min_child_weight = 1, subsample = 0.8 on full training set
[1] "...Done!"
eXtreme Gradient Boosting 

15068 samples
  156 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 13561, 13561, 13562, 13563, 13563, 13561, ... 
Resampling results across tuning parameters:

  nrounds  MAE     
   100     1235.277
   200     1229.159
   300     1233.175
   400     1238.342
   500     1243.464
   600     1249.880
   700     1256.045
   800     1261.477
   900     1264.634
  1000     1270.345

Tuning parameter 'max_depth' was held constant at a value of 5
Tuning
 parameter 'min_child_weight' was held constant at a value of 1

Tuning parameter 'subsample' was held constant at a value of 0.8
MAE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 200, max_depth = 5, eta
 = 0.1, gamma = 0, colsample_bytree = 0.5, min_child_weight = 1 and subsample
 = 0.8. 
[1] "Estimated RMSE:"
        RMSE     Rsquared 
2080.5515929    0.5010928 
[1] "Estimated MAE:"
[1] 1254.021
[1] "Best Parameters:"
  nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
2     200         5 0.1     0              0.5                1       0.8
[1] "Run time:"
   user  system elapsed 
 23.306   2.394 440.223 
